{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP with Spacy (tutorial)\n",
    "\n",
    "This is a notebook (or series of notebooks) made to follow along with the Spacy tutorial course taught by Ines from Explosion AI and Spacy\n",
    "\n",
    "\n",
    "## Chapter 1: Finding words, phrase, names and concepts\n",
    "### Introduction to Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the english language class\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the NLP object; contains pipeline and rules for tokenization, etc.\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# the Doc object; behaves like a sequence\n",
    "doc = nlp('Hello world!')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access a token via its index\n",
    "token = doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# access the token text with the text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the span object; subsets the doc, but doesn't contain data\n",
    "span = doc[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical attributes\n",
    "doc = nlp('It cost me 5 bucks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:     [0, 1, 2, 3, 4, 5]\n",
      "Text:     ['It', 'cost', 'me', '5', 'bucks', '!']\n",
      "is_alpha:  [True, True, True, False, True, False]\n",
      "is_punct:  [False, False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# lexical attributes refer only to vocabulary entry and\n",
    "# do not take into consideration context\n",
    "print('Index:    ', [token.i for token in doc])\n",
    "print('Text:    ', [token.text for token in doc])\n",
    "\n",
    "print('is_alpha: ', [token.is_alpha for token in doc])\n",
    "print('is_punct: ', [token.is_punct for token in doc])\n",
    "print('like_num: ', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download small english model\n",
    "#!python3 -m spacy download en\n",
    "\n",
    "import spacy\n",
    "\n",
    "# load model; contains binary weights, vocabulary\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "a DET\n",
      "whole ADJ\n",
      "pizza NOUN\n",
      "all ADV\n",
      "by ADP\n",
      "herself PRON\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# process text\n",
    "doc = nlp(\"She ate a whole pizza all by herself.\")\n",
    "\n",
    "# print token text and part of speech\n",
    "for token in doc:\n",
    "    # attributes that return strings have trailing underscores\n",
    "    # attribute without underscores are IDs\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "a DET det pizza\n",
      "whole ADJ amod pizza\n",
      "pizza NOUN dobj ate\n",
      "all ADV advmod by\n",
      "by ADP prep ate\n",
      "herself PRON pobj by\n",
      ". PUNCT punct ate\n"
     ]
    }
   ],
   "source": [
    "# predict how words are related\n",
    "for token in doc:\n",
    "    # dep_ returns predicted dependency label\n",
    "    # head returns the head (parent) token\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Barker PERSON\n",
      "Porsche ORG\n",
      "Mars LOC\n"
     ]
    }
   ],
   "source": [
    "# ents are real world objects assigned a name (named entities)\n",
    "# doc.ents property accesses named entities, returns iterator of span objects\n",
    "doc = nlp(u\"Bob Barker drives his Porsche to the Masters on Mars for a million bucks.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People, including fictional'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spacy.explain returns definitions of common tags and labels\n",
    "spacy.explain('PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pronoun'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PRON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob Barker 380\n",
      "Mars 385\n",
      "Elon 380\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Bob Barker drove his Tesla to Mars to meet Elon')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entity: Tesla\n"
     ]
    }
   ],
   "source": [
    "tesla = doc[4:5]\n",
    "print('Missing entity:', tesla.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based matching\n",
    "More powerful than regular expressions; can match on attributes, use predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# initialize the Matcher with shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# match patterns are lists of dictionaries\n",
    "# dictionaries describe tokens; keys are attributes that match to expected values\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# add the pattern\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "doc = nlp('New iPhone X eats a bag of Cheetos')\n",
    "\n",
    "# match the pattern; returns a tuple (match_id, start_index, end_index) for each match\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# create match span object\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {'IS_DIGIT': True}, \n",
    "    {'LOWER': 'fifa'}, \n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'}, \n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "matcher.add('FIFA_WC_PATTERN', None, pattern)\n",
    "doc = nlp('2018 FIFA World Cup: France won!')\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first token has two characteristics (and is followed by a noun):\n",
    "# 1) its lemma is 'love'\n",
    "# 2) it's a verb\n",
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'}, \n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('LOVE_NOUN', None, pattern)\n",
    "doc = nlp('I loved gerbils, but I love cockroaches more now.')\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved gerbils\n",
      "love cockroaches\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start: end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a house\n",
      "buying the world\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'buy'}, \n",
    "    {'POS': 'DET', 'OP': '?'}, # 'OP' makes determiner token optional\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"Devo bought a house and filled it with leather. Now they're buying the world!\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('BUYING_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
